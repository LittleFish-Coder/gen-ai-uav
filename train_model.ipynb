{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/bkkaggle/pytorch-CycleGAN-and-pix2pix/blob/master/pix2pix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"7wNjDKdQy35h"},"source":["# Install"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1204,"status":"ok","timestamp":1710751107262,"user":{"displayName":"D Parejo","userId":"17109521203490422964"},"user_tz":-480},"id":"TRm-USlsHgEV","outputId":"93cc884a-52d9-4541-f9b5-8efe6096f00b"},"outputs":[],"source":["import os\n","\n","# check if `pytorch-CycleGAN-and-pix2pix` is already cloned\n","if not os.path.exists('pytorch-CycleGAN-and-pix2pix'):\n","\t!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"]},{"cell_type":"markdown","metadata":{"id":"jCShEIiiGKtN"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1710751107262,"user":{"displayName":"D Parejo","userId":"17109521203490422964"},"user_tz":-480},"id":"Pt3igws3eiVp"},"outputs":[],"source":["import os\n","os.chdir('pytorch-CycleGAN-and-pix2pix/')"]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":270166,"status":"ok","timestamp":1710751377419,"user":{"displayName":"D Parejo","userId":"17109521203490422964"},"user_tz":-480},"id":"z1EySlOXwwoa","outputId":"c6e39497-644b-4194-8777-d69d971d726e"},"outputs":[],"source":["%pip install -q -r requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"8daqlgVhw29P"},"source":["# Datasets\n","\n","put the dataset in the `datasets` folder\n","\n","Each dataset should have the following directory structure:\n","\n","```\n","datasets\n","└── training_dataset\n","\t├── train\n","\t├── test\n","```"]},{"cell_type":"markdown","metadata":{"id":"yFw1kDQBx3LN"},"source":["# Training\n","\n","-   `python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA`\n","\n","- Change the `--dataroot` and `--name` to your own dataset's path and model's name. \n","\n","- Use `--gpu_ids 0,1,..` to train on multiple GPUs and `--batch_size` to change the batch size."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":219031,"status":"ok","timestamp":1710751642502,"user":{"displayName":"D Parejo","userId":"17109521203490422964"},"user_tz":-480},"id":"0sp7TCT2x9dB","outputId":"dd4dbb3c-69a1-45e6-c307-837571389cff"},"outputs":[],"source":["# the below code train the full dataset in one stage (in which the model is trained on both the road and river images)\n","# use this line as baseline\n","! python train.py --dataroot ./datasets/training_dataset/ --name ROAD_RIVER_pix2pix --model pix2pix --direction AtoB --display_id -1\n","\n","# use nohup to run the training in the background\n","# ! nohup python train.py ... &"]},{"cell_type":"markdown","metadata":{},"source":["# Train 2 domain-specific models\n","- one for RIVER\n","- one for ROAD\n","\n","Add argument:\n","- `--n_epochs=400 (default is 200)`\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ! python train.py --dataroot ./datasets/ROAD --name ROAD --model pix2pix --direction AtoB --display_id 0\n","# ! python train.py --dataroot ./datasets/RIVER --name RIVER --model pix2pix --direction AtoB --display_id 0\n","\n","# use nohup to run the training in the background\n","# ! nohup python train.py ... road.out &\t# for ROAD\n","# ! nohup python train.py ... river.out &\t# for RIVER\n","\n","# Recording use:\n","# [1] 7906\n","# [2] 8494"]},{"cell_type":"markdown","metadata":{"id":"9UkcaFZiyASl"},"source":["# Testing\n","\n","-   `python test.py --dataroot ./datasets/facades --direction BtoA --model pix2pix --name facades_pix2pix`\n","\n","Change the `--dataroot`, `--name`, and `--direction` to be consistent with your trained model's configuration and how you want to transform images.\n","\n","> from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix:\n","> Note that we specified --direction BtoA as Facades dataset's A to B direction is photos to labels.\n","\n","> If you would like to apply a pre-trained model to a collection of input images (rather than image pairs), please use --model test option. See ./scripts/test_single.sh for how to apply a model to Facade label maps (stored in the directory facades/testB).\n","\n","> See a list of currently available models at ./scripts/download_pix2pix_model.sh"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1710751653215,"user":{"displayName":"D Parejo","userId":"17109521203490422964"},"user_tz":-480},"id":"mey7o6j-0368","outputId":"eb13bacc-eb2f-4dd0-9fab-ec33d895f168"},"outputs":[],"source":["# make sure the pre-trained weight is in the checkpoints folder\n","!ls checkpoints/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14188,"status":"ok","timestamp":1710751667388,"user":{"displayName":"D Parejo","userId":"17109521203490422964"},"user_tz":-480},"id":"uCsKkEq0yGh0","outputId":"de377807-1a9f-4aee-adf5-c0e7b93f517b"},"outputs":[],"source":["# public test with whole dataset --num_test 1000\n","! python test.py --dataroot ./datasets/public_testing_dataset_1 --name ROAD_RIVER_pix2pix --model pix2pix --direction AtoB --num_test 1000"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(f\"Size: {len(os.listdir('./results/ROAD_RIVER_pix2pix/test_latest/images/'))}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Transform the results into AI CUP format\n","\n","The results are stored in `./ROAD_pix2pix/test_latest/images/`.\n","\n","And there are 2 types of results: `real_A` and `fake_B`.\n","\n","The `real_A` is the original image, and the `fake_B` is the transformed image.\n","\n","Store the `{Prefix}_fake_B.png` as `{Prefix}.jpg` to `./ROAD_pix2pix/test_latest/submission/`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# store the fake images to the `./results/ROAD_pix2pix/test_latest/submission/` folder\n","\n","import shutil\n","\n","source_folder = './results/ROAD_RIVER_pix2pix/test_latest/images'\n","target_folder = './results/ROAD_RIVER_pix2pix/test_latest/submission'\n","\n","if not os.path.exists(target_folder):\n","    os.makedirs(target_folder)\n","\n","for image_name in os.listdir(source_folder):\n","    if 'fake' in image_name:\n","        shutil.copy(os.path.join(source_folder, image_name), os.path.join(target_folder, image_name))\n","print(f\"Size: {len(os.listdir(target_folder))}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# rename the fake images\n","for image_name in os.listdir(target_folder):\n","    # ignore the postfix: _fake_B.png\n","    # and transform the image from png to jpg\n","    new_name = image_name.replace('_fake_B.png', '.jpg')\n","    os.rename(os.path.join(target_folder, image_name), os.path.join(target_folder, new_name))\n","print(f\"Size: {len(os.listdir(target_folder))}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# zip the fake images\n","shutil.make_archive(target_folder, 'zip', target_folder)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"environment":{"name":"tf2-gpu.2-3.m74","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}
