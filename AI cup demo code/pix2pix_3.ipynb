{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/bkkaggle/pytorch-CycleGAN-and-pix2pix/blob/master/pix2pix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"7wNjDKdQy35h"},"source":["# Install"]},{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nNwV_3sZBRXK","executionInfo":{"status":"ok","timestamp":1710751106078,"user_tz":-480,"elapsed":21637,"user":{"displayName":"D Parejo","userId":"17109521203490422964"}},"outputId":"282d344d-b853-4fb7-b0c0-efaed7cbef3d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TRm-USlsHgEV","executionInfo":{"status":"ok","timestamp":1710751107262,"user_tz":-480,"elapsed":1204,"user":{"displayName":"D Parejo","userId":"17109521203490422964"}},"outputId":"93cc884a-52d9-4541-f9b5-8efe6096f00b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'pytorch-CycleGAN-and-pix2pix'...\n","remote: Enumerating objects: 2513, done.\u001b[K\n","remote: Total 2513 (delta 0), reused 0 (delta 0), pack-reused 2513\u001b[K\n","Receiving objects: 100% (2513/2513), 8.20 MiB | 22.38 MiB/s, done.\n","Resolving deltas: 100% (1575/1575), done.\n"]}],"source":["!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\n","# !git clone https://github.com/NCCU111753/pytorch-CycleGAN-and-pix2pix.git"]},{"cell_type":"markdown","source":[],"metadata":{"id":"jCShEIiiGKtN"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"Pt3igws3eiVp","executionInfo":{"status":"ok","timestamp":1710751107262,"user_tz":-480,"elapsed":10,"user":{"displayName":"D Parejo","userId":"17109521203490422964"}}},"outputs":[],"source":["import os\n","os.chdir('pytorch-CycleGAN-and-pix2pix/')"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"z1EySlOXwwoa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710751377419,"user_tz":-480,"elapsed":270166,"user":{"displayName":"D Parejo","userId":"17109521203490422964"}},"outputId":"c6e39497-644b-4194-8777-d69d971d726e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.2.1+cu121)\n","Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.17.1+cu121)\n","Collecting dominate>=2.4.0 (from -r requirements.txt (line 3))\n","  Downloading dominate-2.9.1-py2.py3-none-any.whl (29 kB)\n","Collecting visdom>=0.1.8.8 (from -r requirements.txt (line 4))\n","  Downloading visdom-0.2.4.tar.gz (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting wandb (from -r requirements.txt (line 5))\n","  Downloading wandb-0.16.4-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (3.13.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (4.10.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.4.0->-r requirements.txt (line 1))\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.4.0->-r requirements.txt (line 1))\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.4.0->-r requirements.txt (line 1))\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.4.0->-r requirements.txt (line 1))\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.4.0->-r requirements.txt (line 1))\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.4.0->-r requirements.txt (line 1))\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.4.0->-r requirements.txt (line 1))\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.4.0->-r requirements.txt (line 1))\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.4.0->-r requirements.txt (line 1))\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.4.0->-r requirements.txt (line 1))\n","  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.4.0->-r requirements.txt (line 1))\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->-r requirements.txt (line 1))\n","  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->-r requirements.txt (line 2)) (1.25.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->-r requirements.txt (line 2)) (9.4.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.11.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (2.31.0)\n","Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (6.3.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.16.0)\n","Collecting jsonpatch (from visdom>=0.1.8.8->-r requirements.txt (line 4))\n","  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n","Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.7.0)\n","Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 5)) (8.1.7)\n","Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 5))\n","  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 5)) (5.9.5)\n","Collecting sentry-sdk>=1.0.0 (from wandb->-r requirements.txt (line 5))\n","  Downloading sentry_sdk-1.42.0-py2.py3-none-any.whl (263 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.5/263.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->-r requirements.txt (line 5))\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 5)) (6.0.1)\n","Collecting setproctitle (from wandb->-r requirements.txt (line 5))\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 5)) (67.7.2)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 5)) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 5)) (3.20.3)\n","Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 5))\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->-r requirements.txt (line 1)) (2.1.5)\n","Collecting jsonpointer>=1.9 (from jsonpatch->visdom>=0.1.8.8->-r requirements.txt (line 4))\n","  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->-r requirements.txt (line 1)) (1.3.0)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 5))\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Building wheels for collected packages: visdom\n","  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for visdom: filename=visdom-0.2.4-py3-none-any.whl size=1408195 sha256=5b713beed2b1348381393bbf0179f5ad07919d889476d93c8a30a8f65b5af087\n","  Stored in directory: /root/.cache/pip/wheels/42/29/49/5bed207bac4578e4d2c0c5fc0226bfd33a7e2953ea56356855\n","Successfully built visdom\n","Installing collected packages: smmap, setproctitle, sentry-sdk, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jsonpointer, dominate, docker-pycreds, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jsonpatch, gitdb, visdom, nvidia-cusolver-cu12, GitPython, wandb\n","Successfully installed GitPython-3.1.42 docker-pycreds-0.4.0 dominate-2.9.1 gitdb-4.0.11 jsonpatch-1.33 jsonpointer-2.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 sentry-sdk-1.42.0 setproctitle-1.3.3 smmap-5.0.1 visdom-0.2.4 wandb-0.16.4\n"]}],"source":["!pip install -r requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"8daqlgVhw29P"},"source":["# Datasets\n","\n","Download one of the official datasets with:\n","\n","-   `bash ./datasets/download_pix2pix_dataset.sh [cityscapes, night2day, edges2handbags, edges2shoes, facades, maps]`\n","\n","Or use your own dataset by creating the appropriate folders and adding in the images. Follow the instructions [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/datasets.md#pix2pix-datasets)."]},{"cell_type":"code","source":["import zipfile\n","import os\n","\n","# 定義要解壓縮的檔案路徑\n","zip_file_path = '/content/drive/MyDrive/Colab_Notebooks/GAN/datasets/ROAD_pix2pix.zip' # your dataset zip file\n","\n","# 定義解壓縮後的目標目錄\n","target_directory = '/content/drive/MyDrive/Colab_Notebooks/GAN/datasets/' # same unzip location\n","\n","# 解壓縮檔案\n","with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n","    zip_ref.extractall(target_directory)\n","\n","# 取得解壓縮後的資料夾名稱\n","extracted_folder_name = os.path.splitext(os.path.basename(zip_file_path))[0]\n","\n","# 最後確保解壓縮後的資料夾名稱與目標目錄名稱一致\n","extracted_folder_path = os.path.join(target_directory, extracted_folder_name)\n","new_folder_path = os.path.join(target_directory, 'ROAD_pix2pix')\n","os.rename(extracted_folder_path, new_folder_path)\n"],"metadata":{"id":"ql3u313PBkhN","executionInfo":{"status":"ok","timestamp":1710751396404,"user_tz":-480,"elapsed":19017,"user":{"displayName":"D Parejo","userId":"17109521203490422964"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import shutil\n","\n","# 定義原始資料夾路徑\n","source_directory = '/content/drive/MyDrive/Colab_Notebooks/GAN/datasets/ROAD_pix2pix' # Gdrive dataset folder\n","\n","# 定義目標目錄\n","target_directory = '/content/pytorch-CycleGAN-and-pix2pix/datasets/' # dest. copy folder\n","\n","# 執行複製操作\n","shutil.copytree(source_directory, os.path.join(target_directory, 'ROAD_pix2pix'))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"62msSMKeDvjU","executionInfo":{"status":"ok","timestamp":1710751423499,"user_tz":-480,"elapsed":27107,"user":{"displayName":"D Parejo","userId":"17109521203490422964"}},"outputId":"e6e5bee1-d64d-4c85-d9c1-5d2fc42fb263"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/pytorch-CycleGAN-and-pix2pix/datasets/ROAD_pix2pix'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","execution_count":7,"metadata":{"id":"vrdOettJxaCc","executionInfo":{"status":"ok","timestamp":1710751423499,"user_tz":-480,"elapsed":29,"user":{"displayName":"D Parejo","userId":"17109521203490422964"}}},"outputs":[],"source":["# !bash ./datasets/download_pix2pix_dataset.sh facades"]},{"cell_type":"markdown","metadata":{"id":"yFw1kDQBx3LN"},"source":["# Training\n","\n","-   `python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA`\n","\n","Change the `--dataroot` and `--name` to your own dataset's path and model's name. Use `--gpu_ids 0,1,..` to train on multiple GPUs and `--batch_size` to change the batch size. Add `--direction BtoA` if you want to train a model to transfrom from class B to A."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"0sp7TCT2x9dB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710751642502,"user_tz":-480,"elapsed":219031,"user":{"displayName":"D Parejo","userId":"17109521203490422964"}},"outputId":"dd4dbb3c-69a1-45e6-c307-837571389cff"},"outputs":[{"output_type":"stream","name":"stdout","text":["----------------- Options ---------------\n","               batch_size: 1                             \n","                    beta1: 0.5                           \n","          checkpoints_dir: ./checkpoints                 \n","           continue_train: False                         \n","                crop_size: 256                           \n","                 dataroot: ./datasets/ROAD_pix2pix       \t[default: None]\n","             dataset_mode: aligned                       \n","                direction: AtoB                          \n","              display_env: main                          \n","             display_freq: 400                           \n","               display_id: 1                             \n","            display_ncols: 4                             \n","             display_port: 8097                          \n","           display_server: http://localhost              \n","          display_winsize: 256                           \n","                    epoch: latest                        \n","              epoch_count: 1                             \n","                 gan_mode: vanilla                       \n","                  gpu_ids: 0                             \n","                init_gain: 0.02                          \n","                init_type: normal                        \n","                 input_nc: 3                             \n","                  isTrain: True                          \t[default: None]\n","                lambda_L1: 100.0                         \n","                load_iter: 0                             \t[default: 0]\n","                load_size: 286                           \n","                       lr: 0.0002                        \n","           lr_decay_iters: 50                            \n","                lr_policy: linear                        \n","         max_dataset_size: inf                           \n","                    model: pix2pix                       \t[default: cycle_gan]\n","                 n_epochs: 100                           \n","           n_epochs_decay: 100                           \n","               n_layers_D: 3                             \n","                     name: ROAD_pix2pix                  \t[default: experiment_name]\n","                      ndf: 64                            \n","                     netD: basic                         \n","                     netG: unet_256                      \n","                      ngf: 64                            \n","               no_dropout: False                         \n","                  no_flip: False                         \n","                  no_html: False                         \n","                     norm: batch                         \n","              num_threads: 4                             \n","                output_nc: 3                             \n","                    phase: train                         \n","                pool_size: 0                             \n","               preprocess: resize_and_crop               \n","               print_freq: 100                           \n","             save_by_iter: False                         \n","          save_epoch_freq: 5                             \n","         save_latest_freq: 5000                          \n","           serial_batches: False                         \n","                   suffix:                               \n","         update_html_freq: 1000                          \n","                use_wandb: False                         \n","                  verbose: False                         \n","       wandb_project_name: CycleGAN-and-pix2pix          \n","----------------- End -------------------\n","dataset [AlignedDataset] was created\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","The number of training images = 1739\n","initialize network with normal\n","initialize network with normal\n","model [Pix2PixModel] was created\n","---------- Networks initialized -------------\n","[Network G] Total number of parameters : 54.414 M\n","[Network D] Total number of parameters : 2.769 M\n","-----------------------------------------------\n","Setting up a new session...\n","Exception in user code:\n","------------------------------------------------------------\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 203, in _new_conn\n","    sock = connection.create_connection(\n","  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/connection.py\", line 85, in create_connection\n","    raise err\n","  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/connection.py\", line 73, in create_connection\n","    sock.connect(sa)\n","ConnectionRefusedError: [Errno 111] Connection refused\n","\n","The above exception was the direct cause of the following exception:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 791, in urlopen\n","    response = self._make_request(\n","  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 497, in _make_request\n","    conn.request(\n","  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 395, in request\n","    self.endheaders()\n","  File \"/usr/lib/python3.10/http/client.py\", line 1278, in endheaders\n","    self._send_output(message_body, encode_chunked=encode_chunked)\n","  File \"/usr/lib/python3.10/http/client.py\", line 1038, in _send_output\n","    self.send(msg)\n","  File \"/usr/lib/python3.10/http/client.py\", line 976, in send\n","    self.connect()\n","  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 243, in connect\n","    self.sock = self._new_conn()\n","  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 218, in _new_conn\n","    raise NewConnectionError(\n","urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x78870271d2a0>: Failed to establish a new connection: [Errno 111] Connection refused\n","\n","The above exception was the direct cause of the following exception:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 486, in send\n","    resp = conn.urlopen(\n","  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 845, in urlopen\n","    retries = retries.increment(\n","  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/retry.py\", line 515, in increment\n","    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n","urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=8097): Max retries exceeded with url: /env/main (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x78870271d2a0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/visdom/__init__.py\", line 756, in _send\n","    return self._handle_post(\n","  File \"/usr/local/lib/python3.10/dist-packages/visdom/__init__.py\", line 720, in _handle_post\n","    r = self.session.post(url, data=data)\n","  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 637, in post\n","    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 589, in request\n","    resp = self.send(prep, **send_kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 703, in send\n","    r = adapter.send(request, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 519, in send\n","    raise ConnectionError(e, request=request)\n","requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8097): Max retries exceeded with url: /env/main (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x78870271d2a0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n","[Errno 99] Cannot assign requested address\n","Visdom.setup_socket.<locals>.on_close() takes 1 positional argument but 3 were given\n","[Errno 99] Cannot assign requested address\n","Visdom.setup_socket.<locals>.on_close() takes 1 positional argument but 3 were given\n","[Errno 99] Cannot assign requested address\n","Visdom.setup_socket.<locals>.on_close() takes 1 positional argument but 3 were given\n","Visdom python client failed to establish socket to get messages from the server. This feature is optional and can be disabled by initializing Visdom with `use_incoming_socket=False`, which will prevent waiting for this request to timeout.\n","\n","\n","Could not connect to Visdom server. \n"," Trying to start a server....\n","Command: /usr/bin/python3 -m visdom.server -p 8097 &>/dev/null &\n","create web directory ./checkpoints/ROAD_pix2pix/web...\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n","learning rate 0.0002000 -> 0.0002000\n","[Errno 99] Cannot assign requested address\n","Visdom.setup_socket.<locals>.on_close() takes 1 positional argument but 3 were given\n","(epoch: 1, iters: 100, time: 0.083, data: 0.237) G_GAN: 3.241 G_L1: 42.265 D_real: 0.046 D_fake: 0.078 \n","(epoch: 1, iters: 200, time: 0.069, data: 0.002) G_GAN: 3.151 G_L1: 37.314 D_real: 0.122 D_fake: 0.126 \n","(epoch: 1, iters: 300, time: 0.087, data: 0.009) G_GAN: 4.078 G_L1: 45.034 D_real: 0.025 D_fake: 0.034 \n","(epoch: 1, iters: 400, time: 0.230, data: 0.002) G_GAN: 2.393 G_L1: 52.171 D_real: 0.007 D_fake: 0.587 \n","(epoch: 1, iters: 500, time: 0.079, data: 0.002) G_GAN: 3.171 G_L1: 34.807 D_real: 0.408 D_fake: 0.036 \n","(epoch: 1, iters: 600, time: 0.087, data: 0.009) G_GAN: 2.192 G_L1: 27.927 D_real: 0.275 D_fake: 0.160 \n","(epoch: 1, iters: 700, time: 0.075, data: 0.002) G_GAN: 2.714 G_L1: 29.208 D_real: 0.011 D_fake: 0.825 \n","(epoch: 1, iters: 800, time: 0.138, data: 0.009) G_GAN: 4.439 G_L1: 34.666 D_real: 0.026 D_fake: 0.038 \n","(epoch: 1, iters: 900, time: 0.090, data: 0.002) G_GAN: 2.458 G_L1: 17.984 D_real: 0.207 D_fake: 0.096 \n","(epoch: 1, iters: 1000, time: 0.082, data: 0.002) G_GAN: 2.497 G_L1: 21.944 D_real: 0.176 D_fake: 0.201 \n","(epoch: 1, iters: 1100, time: 0.090, data: 0.002) G_GAN: 2.369 G_L1: 21.273 D_real: 0.217 D_fake: 0.104 \n","(epoch: 1, iters: 1200, time: 0.151, data: 0.002) G_GAN: 3.061 G_L1: 48.099 D_real: 0.001 D_fake: 0.074 \n","(epoch: 1, iters: 1300, time: 0.090, data: 0.002) G_GAN: 2.756 G_L1: 42.237 D_real: 0.001 D_fake: 1.034 \n","(epoch: 1, iters: 1400, time: 0.092, data: 0.002) G_GAN: 2.212 G_L1: 24.690 D_real: 1.690 D_fake: 0.030 \n","(epoch: 1, iters: 1500, time: 0.080, data: 0.002) G_GAN: 3.010 G_L1: 39.925 D_real: 0.001 D_fake: 0.087 \n","(epoch: 1, iters: 1600, time: 0.149, data: 0.016) G_GAN: 3.134 G_L1: 34.826 D_real: 0.003 D_fake: 0.110 \n","(epoch: 1, iters: 1700, time: 0.078, data: 0.002) G_GAN: 4.078 G_L1: 57.425 D_real: 0.002 D_fake: 0.032 \n","End of epoch 1 / 200 \t Time Taken: 101 sec\n","learning rate 0.0002000 -> 0.0002000\n","(epoch: 2, iters: 61, time: 0.093, data: 0.002) G_GAN: 3.825 G_L1: 31.676 D_real: 0.525 D_fake: 0.019 \n","(epoch: 2, iters: 161, time: 0.094, data: 0.002) G_GAN: 1.874 G_L1: 24.965 D_real: 0.048 D_fake: 0.258 \n","(epoch: 2, iters: 261, time: 0.205, data: 0.002) G_GAN: 3.829 G_L1: 30.749 D_real: 0.057 D_fake: 0.034 \n","(epoch: 2, iters: 361, time: 0.094, data: 0.002) G_GAN: 1.418 G_L1: 28.043 D_real: 0.003 D_fake: 0.325 \n","(epoch: 2, iters: 461, time: 0.088, data: 0.002) G_GAN: 3.488 G_L1: 32.366 D_real: 0.006 D_fake: 0.052 \n","(epoch: 2, iters: 561, time: 0.092, data: 0.009) G_GAN: 3.703 G_L1: 38.177 D_real: 0.001 D_fake: 0.036 \n","(epoch: 2, iters: 661, time: 0.148, data: 0.002) G_GAN: 3.371 G_L1: 41.157 D_real: 0.016 D_fake: 0.042 \n","(epoch: 2, iters: 761, time: 0.092, data: 0.002) G_GAN: 3.216 G_L1: 37.359 D_real: 0.003 D_fake: 0.066 \n","(epoch: 2, iters: 861, time: 0.091, data: 0.002) G_GAN: 4.100 G_L1: 54.138 D_real: 0.002 D_fake: 0.025 \n","(epoch: 2, iters: 961, time: 0.072, data: 0.002) G_GAN: 2.449 G_L1: 29.275 D_real: 0.004 D_fake: 0.643 \n","(epoch: 2, iters: 1061, time: 0.150, data: 0.003) G_GAN: 3.993 G_L1: 49.704 D_real: 0.008 D_fake: 0.057 \n","(epoch: 2, iters: 1161, time: 0.074, data: 0.002) G_GAN: 3.662 G_L1: 29.804 D_real: 0.520 D_fake: 0.026 \n","(epoch: 2, iters: 1261, time: 0.092, data: 0.016) G_GAN: 2.034 G_L1: 22.360 D_real: 1.274 D_fake: 0.067 \n","(epoch: 2, iters: 1361, time: 0.091, data: 0.002) G_GAN: 1.366 G_L1: 25.020 D_real: 0.321 D_fake: 0.110 \n","(epoch: 2, iters: 1461, time: 0.144, data: 0.002) G_GAN: 3.745 G_L1: 26.461 D_real: 0.044 D_fake: 0.028 \n","(epoch: 2, iters: 1561, time: 0.093, data: 0.002) G_GAN: 2.137 G_L1: 28.388 D_real: 1.215 D_fake: 0.092 \n","(epoch: 2, iters: 1661, time: 0.075, data: 0.002) G_GAN: 3.190 G_L1: 32.553 D_real: 0.528 D_fake: 0.037 \n","End of epoch 2 / 200 \t Time Taken: 96 sec\n","learning rate 0.0002000 -> 0.0002000\n","(epoch: 3, iters: 22, time: 0.091, data: 0.013) G_GAN: 4.562 G_L1: 36.504 D_real: 0.042 D_fake: 0.013 \n","(epoch: 3, iters: 122, time: 0.210, data: 0.002) G_GAN: 3.585 G_L1: 33.627 D_real: 0.004 D_fake: 0.044 \n","Traceback (most recent call last):\n","  File \"/content/pytorch-CycleGAN-and-pix2pix/train.py\", line 52, in <module>\n","    model.optimize_parameters()   # calculate loss functions, get gradients, update network weights\n","  File \"/content/pytorch-CycleGAN-and-pix2pix/models/pix2pix_model.py\", line 122, in optimize_parameters\n","    self.optimizer_D.step()          # update D's weights\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py\", line 75, in wrapper\n","    return wrapped(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\", line 385, in wrapper\n","    out = func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\", line 76, in _use_grad\n","    ret = func(self, *args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\", line 166, in step\n","    adam(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\", line 316, in adam\n","    func(params,\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\", line 523, in _multi_tensor_adam\n","    torch._foreach_addcmul_(device_exp_avg_sqs, device_grads, device_grads, 1 - beta2)\n","KeyboardInterrupt\n"]}],"source":["# !python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA --display_id -1\n","\n","! python train.py --dataroot ./datasets/ROAD_pix2pix --name ROAD_pix2pix --model pix2pix --direction AtoB\n"]},{"cell_type":"markdown","metadata":{"id":"gdUz4116xhpm"},"source":["# Pretrained models\n","\n","Download one of the official pretrained models with:\n","\n","-   `bash ./scripts/download_pix2pix_model.sh [edges2shoes, sat2map, map2sat, facades_label2photo, and day2night]`\n","\n","Or add your own pretrained model to `./checkpoints/{NAME}_pretrained/latest_net_G.pt`"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"GC2DEP4M0OsS","executionInfo":{"status":"ok","timestamp":1710751642502,"user_tz":-480,"elapsed":17,"user":{"displayName":"D Parejo","userId":"17109521203490422964"}}},"outputs":[],"source":["# !bash ./scripts/download_pix2pix_model.sh facades_label2photo"]},{"cell_type":"code","source":["import shutil\n","\n","# 定義原始檔案路徑\n","source_file = '/content/drive/MyDrive/Colab_Notebooks/GAN/checkpoints/ROAD_pix2pix/latest_net_G.pth'\n","\n","# 定義目標目錄\n","target_directory = '/content/pytorch-CycleGAN-and-pix2pix/checkpoints/ROAD_pix2pix/'\n","\n","# 執行複製操作\n","shutil.copy(source_file, target_directory)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"E2Vk1PXOP0iM","executionInfo":{"status":"ok","timestamp":1710751653214,"user_tz":-480,"elapsed":10726,"user":{"displayName":"D Parejo","userId":"17109521203490422964"}},"outputId":"693e0212-610a-4f8e-9434-84989c71e9a0"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/pytorch-CycleGAN-and-pix2pix/checkpoints/ROAD_pix2pix/latest_net_G.pth'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"9UkcaFZiyASl"},"source":["# Testing\n","\n","-   `python test.py --dataroot ./datasets/facades --direction BtoA --model pix2pix --name facades_pix2pix`\n","\n","Change the `--dataroot`, `--name`, and `--direction` to be consistent with your trained model's configuration and how you want to transform images.\n","\n","> from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix:\n","> Note that we specified --direction BtoA as Facades dataset's A to B direction is photos to labels.\n","\n","> If you would like to apply a pre-trained model to a collection of input images (rather than image pairs), please use --model test option. See ./scripts/test_single.sh for how to apply a model to Facade label maps (stored in the directory facades/testB).\n","\n","> See a list of currently available models at ./scripts/download_pix2pix_model.sh"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"mey7o6j-0368","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710751653215,"user_tz":-480,"elapsed":20,"user":{"displayName":"D Parejo","userId":"17109521203490422964"}},"outputId":"eb13bacc-eb2f-4dd0-9fab-ec33d895f168"},"outputs":[{"output_type":"stream","name":"stdout","text":["ROAD_pix2pix\n"]}],"source":["!ls checkpoints/"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"uCsKkEq0yGh0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710751667388,"user_tz":-480,"elapsed":14188,"user":{"displayName":"D Parejo","userId":"17109521203490422964"}},"outputId":"de377807-1a9f-4aee-adf5-c0e7b93f517b"},"outputs":[{"output_type":"stream","name":"stdout","text":["----------------- Options ---------------\n","             aspect_ratio: 1.0                           \n","               batch_size: 1                             \n","          checkpoints_dir: ./checkpoints                 \n","                crop_size: 256                           \n","                 dataroot: ./datasets/ROAD_pix2pix       \t[default: None]\n","             dataset_mode: aligned                       \n","                direction: AtoB                          \n","          display_winsize: 256                           \n","                    epoch: latest                        \n","                     eval: False                         \n","                  gpu_ids: 0                             \n","                init_gain: 0.02                          \n","                init_type: normal                        \n","                 input_nc: 3                             \n","                  isTrain: False                         \t[default: None]\n","                load_iter: 0                             \t[default: 0]\n","                load_size: 256                           \n","         max_dataset_size: inf                           \n","                    model: pix2pix                       \t[default: test]\n","               n_layers_D: 3                             \n","                     name: ROAD_pix2pix                  \t[default: experiment_name]\n","                      ndf: 64                            \n","                     netD: basic                         \n","                     netG: unet_256                      \n","                      ngf: 64                            \n","               no_dropout: False                         \n","                  no_flip: False                         \n","                     norm: batch                         \n","                 num_test: 50                            \n","              num_threads: 4                             \n","                output_nc: 3                             \n","                    phase: test                          \n","               preprocess: resize_and_crop               \n","              results_dir: ./results/                    \n","           serial_batches: False                         \n","                   suffix:                               \n","                use_wandb: False                         \n","                  verbose: False                         \n","       wandb_project_name: CycleGAN-and-pix2pix          \n","----------------- End -------------------\n","dataset [AlignedDataset] was created\n","initialize network with normal\n","model [Pix2PixModel] was created\n","loading the model from ./checkpoints/ROAD_pix2pix/latest_net_G.pth\n","---------- Networks initialized -------------\n","[Network G] Total number of parameters : 54.414 M\n","-----------------------------------------------\n","creating web directory ./results/ROAD_pix2pix/test_latest\n","processing (0000)-th image... ['./datasets/ROAD_pix2pix/test/road_cloudy_90_30_final_0291.jpg']\n","processing (0005)-th image... ['./datasets/ROAD_pix2pix/test/road_cloudy_90_30_final_0296.jpg']\n","processing (0010)-th image... ['./datasets/ROAD_pix2pix/test/road_cloudy_90_60_final_0291.jpg']\n","processing (0015)-th image... ['./datasets/ROAD_pix2pix/test/road_cloudy_90_60_final_0296.jpg']\n","processing (0020)-th image... ['./datasets/ROAD_pix2pix/test/road_rain_90_30_final_0291.jpg']\n","processing (0025)-th image... ['./datasets/ROAD_pix2pix/test/road_rain_90_30_final_0296.jpg']\n","processing (0030)-th image... ['./datasets/ROAD_pix2pix/test/road_rain_90_60_final_0291.jpg']\n","processing (0035)-th image... ['./datasets/ROAD_pix2pix/test/road_rain_90_60_final_0296.jpg']\n","processing (0040)-th image... ['./datasets/ROAD_pix2pix/test/road_sunny_90_30_final_0291.jpg']\n","processing (0045)-th image... ['./datasets/ROAD_pix2pix/test/road_sunny_90_30_final_0296.jpg']\n"]}],"source":["# !python test.py --dataroot ./datasets/facades --direction BtoA --model pix2pix --name facades_label2photo_pretrained --use_wandb\n","! python test.py --dataroot ./datasets/ROAD_pix2pix --name ROAD_pix2pix --model pix2pix --direction AtoB\n"]},{"cell_type":"markdown","metadata":{"id":"OzSKIPUByfiN"},"source":["# Visualize"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"9Mgg8raPyizq","executionInfo":{"status":"ok","timestamp":1710751667389,"user_tz":-480,"elapsed":143,"user":{"displayName":"D Parejo","userId":"17109521203490422964"}}},"outputs":[],"source":["# import matplotlib.pyplot as plt\n","\n","# img = plt.imread('./results/facades_label2photo_pretrained/test_latest/images/100_fake_B.png')\n","# img = plt.imread('./results/ROAD_pix2pix/test_latest/images/road_cloudy_90_30_final_0291_fake_B.png')\n","# plt.imshow(img)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"0G3oVH9DyqLQ","executionInfo":{"status":"ok","timestamp":1710751667389,"user_tz":-480,"elapsed":128,"user":{"displayName":"D Parejo","userId":"17109521203490422964"}}},"outputs":[],"source":["# img = plt.imread('./results/facades_label2photo_pretrained/test_latest/images/100_real_A.png')\n","# plt.imshow(img)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"ErK5OC1j1LH4","executionInfo":{"status":"ok","timestamp":1710751667389,"user_tz":-480,"elapsed":121,"user":{"displayName":"D Parejo","userId":"17109521203490422964"}}},"outputs":[],"source":["# img = plt.imread('./results/facades_label2photo_pretrained/test_latest/images/100_real_B.png')\n","# plt.imshow(img)"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"environment":{"name":"tf2-gpu.2-3.m74","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}